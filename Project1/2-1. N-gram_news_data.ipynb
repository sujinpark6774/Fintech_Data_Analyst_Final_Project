{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from numpy.random import randint, seed\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from PIL import Image  \n",
    "from IPython.display import set_matplotlib_formats\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\workspace\\\\Python\\\\4-2) 핀테크 교육\\\\Final Project\\\\Project1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\workspace\\\\Python\\\\4-2) 핀테크 교육\\\\Final Project\\\\Project1\\\\data\\\\total'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(r\".\\data\\total\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13738, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text=pd.read_excel(\"교양오락문화생활비.xlsx\", encoding='CP949')\n",
    "my_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['김준호 유튜브 채널 구독자 38만명 돌파…뼈그맨 인증',\n",
       "       '\\'두데\\' 감스트 \"구독자들 \\'야\\'라고 불러…난 \\'인직아\\'라고 불린다\"',\n",
       "       '[뉴스레터 구독 이벤트] 1차 당첨자 추첨 과정 공개…100명의 행운의 주인공은...',\n",
       "       '[MT리포트] \"1년 수입 10억\"…수백만 구독자 \\'유튜버의 세계\\'',\n",
       "       'CJ E&M, ‘다이아 티비’ 구독자 1억 6000만명 돌파',\n",
       "       \"'슈가맨' '히든싱어' 등 음악예능 인기에…JTBC 유튜브 구독자 200만 돌파\",\n",
       "       \"250만 구독자 '유튜버 망치'한국의 맛 알려\",\n",
       "       \"[뉴스레터 구독 이벤트] RC카 '러커스'로 대리만족 한 번 해보시렵니까\",\n",
       "       \"[뉴스레터 구독 이벤트] 1차를 놓쳤다면 'V30'가 기다리는 2차 이벤트에 도전\",\n",
       "       '250만 구독자 유튜버가 전하는 한국의 맛'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text['title'].values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "772"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_text['title'].values) - len(set(my_text['title'].values)) # 중복 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_meaning = \"구독|책|핫플레이스|핫|플레이스|레이스|휴가비\"\n",
    "my_text_clean = []\n",
    "\n",
    "for a_line in set(my_text['title'].values):\n",
    "    a_line = re.sub('[^ㄱ-ㅣ가-힣]+', ' ', a_line) # 한글, 숫자 이외는 모두 공백으로\n",
    "    a_line = re.sub(no_meaning, '', a_line)           # 의미없는 문자 삭제\n",
    "    a_line = re.sub('\\s+', ' ', a_line)               # 잉여 스페이스를 공백 하나로\n",
    "    my_text_clean += [a_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['서울시교육청 은혜초 무단폐교 이사장 고발 교장은 해임 요청',\n",
       " '다시 통상임금을 묻다',\n",
       " '경남과기대 용역근로자에 임명장 수여',\n",
       " ' 과 길 외국어 란 렌즈를 통해 본 인류 ',\n",
       " ' 너도 인간이니 공승연 인공지능 사랑 할 수 있어 ',\n",
       " ' 김비서가 왜 그럴까 박서준 박민영 심야 놀이공원 데이트 달달 로맨틱 ',\n",
       " '커넥츠북 매일 매일 드림 명 추첨 공짜로',\n",
       " '최순실 메모 보니 특활비 배분에 개입',\n",
       " ' 이동진의 빨간방 인터뷰 방식 때문에 의미도 상당한 ',\n",
       " ' 꽂이 새 자 코끼리의 시간 쥐의 시간 ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text_clean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = konlpy.tag.Hannanum()\n",
    "\n",
    "# 명사 추출\n",
    "my_words = []\n",
    "for a_line, n in zip(my_text_clean, range(len(my_text_clean))):\n",
    "    #print(n, end=\" \")\n",
    "    if a_line == \" \":\n",
    "        #print(\"패스\")\n",
    "        continue\n",
    "    my_words += hannanum.nouns(a_line)\n",
    "    \n",
    "#my_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단음절 제거\n",
    "my_words_2 = [\"\"]\n",
    "for a_word in my_words:\n",
    "    if len(a_word) > 1:\n",
    "        my_words_2[0] += \" \" + a_word\n",
    "        \n",
    "#my_words_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2                                                            # Can be changed to a number equal or larger than 2.\n",
    "n_min = n\n",
    "n_max = n\n",
    "n_gram_type = 'word'                                             # n-Gram with words.\n",
    "vectorizer = CountVectorizer(ngram_range=(n_min,n_max), analyzer = n_gram_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()            # Get the n-Grams as a list.\n",
    "n_gram_cts = vectorizer.transform(my_words_2).toarray()             #  The output is an array of array.\n",
    "n_gram_cts = list(n_gram_cts[0])                                 # Convert into a simple list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('정규직 전환', 103),\n",
       " ('국정원 특활비', 73),\n",
       " ('근로자 휴가지원사업', 60),\n",
       " ('만명 돌파', 56),\n",
       " ('임금협약 체결', 50),\n",
       " ('박서준 박민영', 42),\n",
       " ('김비서 박서준', 35),\n",
       " ('박근혜 국정원', 34),\n",
       " ('노사 임단협', 33),\n",
       " ('만원 지원', 33)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict = {}\n",
    "for z, zz in zip(n_grams, n_gram_cts):\n",
    "    res_dict[z] = zz\n",
    "res_dict = sorted(res_dict.items(), key=(lambda x: x[1]), reverse=True)\n",
    "res_dict[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다음 연관 단어 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 교양오락문화생활비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    n = 0\n",
    "    print(text, \"--->\", end=\" \")\n",
    "    for i in range(len(res_dict)):\n",
    "        if text == res_dict[i][0].split(\" \")[0]:\n",
    "            print(res_dict[i][0].split(\" \")[1], end=\" \")\n",
    "            n += 1\n",
    "        if n == 5:\n",
    "            break\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_arr(arr):\n",
    "    n = range(len(arr))\n",
    "    for i, j in zip(arr, n):\n",
    "        print(j+1, \"위 : \", end=\"\")\n",
    "        predict(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 위 : 여행 ---> 코스 어디 이야기 이태 향기 \n",
      "2 위 : 맛집 ---> 기행 대천왕 샤이바 경기 공개 \n",
      "3 위 : 종합 ---> 밤도깨비 포토 검찰 김비서 숲속 \n",
      "4 위 : 오늘 ---> 첫방 박근혜 세계 방송 공개 \n",
      "5 위 : 개최 ---> 금감원 대통령 밤도깨비 여름 연휴 \n",
      "6 위 : 서울 ---> 성수동 강남대로 상권 강남 마지막 \n",
      "7 위 : 부산 ---> 보이스피싱 관광 기장 송도구름산 국제금융센터 \n",
      "8 위 : 올해 ---> 선정 선포식 특집 인기 협력사 \n",
      "9 위 : 제주 ---> 호텔 동네방 땅값 중국인 가동 \n",
      "10 위 : 공개 ---> 밤도깨비 삼성 스포티한 이재명 가을하늘 \n"
     ]
    }
   ],
   "source": [
    "predict_arr(['여행', '맛집', '종합', '오늘', '개최', \n",
    "           '서울', '부산', '올해', '제주', '공개'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 교통통신비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = pd.read_excel(\"교통통신비.xlsx\", encoding='CP949')\n",
    "\n",
    "no_meaning = \"교통비|대중교통|통신비\"\n",
    "my_text_clean = []\n",
    "\n",
    "for a_line in set(my_text['title'].values):\n",
    "    a_line = re.sub('[^ ㄱ-ㅣ가-힣0-9]+', ' ', a_line) # 한글, 숫자 이외는 모두 공백으로\n",
    "    a_line = re.sub(no_meaning, '', a_line)            # 의미없는 문자 삭제\n",
    "    a_line = re.sub('\\s+', ' ', a_line)                # 잉여 스페이스를 공백 하나로\n",
    "    my_text_clean += [a_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = konlpy.tag.Hannanum()\n",
    "\n",
    "my_words = []\n",
    "for a_line, n in zip(my_text_clean, range(len(my_text_clean))):\n",
    "    #print(n, end=\" \")\n",
    "    if a_line == \" \":\n",
    "        #print(\"패스\")\n",
    "        continue\n",
    "    my_words += hannanum.nouns(a_line)\n",
    "\n",
    "my_words_2 = [\"\"]\n",
    "for a_word in my_words:\n",
    "    if len(a_word) > 1:\n",
    "        my_words_2[0] += \" \" + a_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('최저임금 산입범위', 138),\n",
       " ('서울 무료', 122),\n",
       " ('출퇴근 무료', 101),\n",
       " ('가계 정책협의회', 96),\n",
       " ('미세먼지 무료', 93),\n",
       " ('서울시 미세먼지', 90),\n",
       " ('미세먼지 비상저감조치', 88),\n",
       " ('보편요금제 도입', 84),\n",
       " ('무료 미세먼지', 70),\n",
       " ('서울시 무료', 70)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2                                                            # Can be changed to a number equal or larger than 2.\n",
    "n_min = n\n",
    "n_max = n\n",
    "n_gram_type = 'word'                                             # n-Gram with words.\n",
    "vectorizer = CountVectorizer(ngram_range=(n_min,n_max), analyzer = n_gram_type)\n",
    "\n",
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()            # Get the n-Grams as a list.\n",
    "n_gram_cts = vectorizer.transform(my_words_2).toarray()             #  The output is an array of array.\n",
    "n_gram_cts = list(n_gram_cts[0])                                 # Convert into a simple list.\n",
    "\n",
    "res_dict = {}\n",
    "for z, zz in zip(n_grams, n_gram_cts):\n",
    "    res_dict[z] = zz\n",
    "res_dict = sorted(res_dict.items(), key=(lambda x: x[1]), reverse=True)\n",
    "res_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    n = 0\n",
    "    print(text, \"--->\", end=\" \")\n",
    "    for i in range(len(res_dict)):\n",
    "        if text == res_dict[i][0].split(\" \")[0]:\n",
    "            print(res_dict[i][0].split(\" \")[1], end=\" \")\n",
    "            n += 1\n",
    "        if n == 5:\n",
    "            break\n",
    "    print(\"\")\n",
    "\n",
    "def predict_arr(arr):\n",
    "    n = range(len(arr))\n",
    "    for i, j in zip(arr, n):\n",
    "        print(j+1, \"위 : \", end=\"\")\n",
    "        predict(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 위 : 지원 ---> 확대 공약 미세먼지 최저임금 보편요금제 \n",
      "2 위 : 보편요금제 ---> 도입 규개위 국회 이통사 심사 \n",
      "3 위 : 정부 ---> 이통사 보편요금제 1년 청년 압박 \n",
      "4 위 : 이통사 ---> 보편요금제 요금제 정부 고가요금제 알뜰폰 \n",
      "5 위 : 추진 ---> 보편요금제 서울시 종합 최저임금 경기 \n",
      "6 위 : 이통3사 ---> 보편요금제 갤럭시 실적 씽큐 요금제 \n",
      "7 위 : 이용 ---> 미세먼지 캠페인 당부 싼커 가능 \n",
      "8 위 : 종합 ---> 최저임금 미세먼지 가계 보편요금제 텔레콤 \n",
      "9 위 : 최저임금 ---> 산입범위 인상 산입 상여금 개정안 \n",
      "10 위 : 서울 ---> 무료 출퇴근 미세먼지 버스 하루 \n"
     ]
    }
   ],
   "source": [
    "predict_arr(['지원', '보편요금제', '정부', '이통사', '추진',\n",
    "          '이통3사', '이용', '종합', '최저임금', '서울'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 내구재비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = pd.read_excel(\"내구재비.xlsx\", encoding='CP949')\n",
    "\n",
    "no_meaning = \"가전제품|가전|내구재|전자제품|전자|제품\"\n",
    "my_text_clean = []\n",
    "\n",
    "for a_line in set(my_text['title'].values):\n",
    "    a_line = re.sub('[^ ㄱ-ㅣ가-힣]+', ' ', a_line) # 한글, 숫자 이외는 모두 공백으로\n",
    "    a_line = re.sub(no_meaning, '', a_line)            # 의미없는 문자 삭제\n",
    "    a_line = re.sub('\\s+', ' ', a_line)                # 잉여 스페이스를 공백 하나로\n",
    "    my_text_clean += [a_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = konlpy.tag.Hannanum()\n",
    "\n",
    "my_words = []\n",
    "for a_line, n in zip(my_text_clean, range(len(my_text_clean))):\n",
    "    #print(n, end=\" \")\n",
    "    if a_line == \" \":\n",
    "        #print(\"패스\")\n",
    "        continue\n",
    "    my_words += hannanum.nouns(a_line)\n",
    "\n",
    "my_words_2 = [\"\"]\n",
    "for a_word in my_words:\n",
    "    if len(a_word) > 1:\n",
    "        my_words_2[0] += \" \" + a_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('생산 소비', 113),\n",
       " ('시장 공략', 97),\n",
       " ('소비 트리플', 56),\n",
       " ('출시 삼성', 50),\n",
       " ('최고 대형', 47),\n",
       " ('삼성 건조기', 42),\n",
       " ('올해 최고', 38),\n",
       " ('삼성 빅스비', 35),\n",
       " ('개월 연속', 34),\n",
       " ('국내 출시', 33)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2                                                            # Can be changed to a number equal or larger than 2.\n",
    "n_min = n\n",
    "n_max = n\n",
    "n_gram_type = 'word'                                             # n-Gram with words.\n",
    "vectorizer = CountVectorizer(ngram_range=(n_min,n_max), analyzer = n_gram_type)\n",
    "\n",
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()            # Get the n-Grams as a list.\n",
    "n_gram_cts = vectorizer.transform(my_words_2).toarray()             #  The output is an array of array.\n",
    "n_gram_cts = list(n_gram_cts[0])                                 # Convert into a simple list.\n",
    "\n",
    "res_dict = {}\n",
    "for z, zz in zip(n_grams, n_gram_cts):\n",
    "    res_dict[z] = zz\n",
    "res_dict = sorted(res_dict.items(), key=(lambda x: x[1]), reverse=True)\n",
    "res_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    n = 0\n",
    "    print(text, \"--->\", end=\" \")\n",
    "    for i in range(len(res_dict)):\n",
    "        if text == res_dict[i][0].split(\" \")[0]:\n",
    "            print(res_dict[i][0].split(\" \")[1], end=\" \")\n",
    "            n += 1\n",
    "        if n == 5:\n",
    "            break\n",
    "    print(\"\")\n",
    "    \n",
    "def predict_arr(arr):\n",
    "    n = range(len(arr))\n",
    "    for i, j in zip(arr, n):\n",
    "        print(j+1, \"위 : \", end=\"\")\n",
    "        predict(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 위 : 삼성 ---> 건조기 빅스비 갤럭시 사장 샘표 \n",
      "2 위 : 출시 ---> 삼성 대우 스마트홈 만원 수출 \n",
      "3 위 : 종합 ---> 삼성 생산 미세먼지 기재부 김현석 \n",
      "4 위 : 스마트폰 ---> 시장 씽큐 보상 분기 출시 \n",
      "5 위 : 미세먼지 ---> 관련 영향 황사 분기 공기청정기 \n",
      "6 위 : 생산 ---> 소비 증가세 반도체 개월 조정 \n",
      "7 위 : 공개 ---> 삼성 중국 대유위니 생산 정부 \n",
      "8 위 : 수출 ---> 호조 제조업 중심 감소 생산 \n",
      "9 위 : 시장 ---> 공략 진출 삼성 연속 정조준 \n",
      "10 위 : 올해 ---> 최고 분기 소비 경제성장률 성장률 \n"
     ]
    }
   ],
   "source": [
    "predict_arr(['삼성', '출시', '종합', '스마트폰', '미세먼지',\n",
    "          '생산', '공개', '수출', '시장', '올해'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 외식비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = pd.read_excel(\"외식비.xlsx\", encoding='CP949')\n",
    "\n",
    "no_meaning = \"외식|외식비|업계\"\n",
    "my_text_clean = []\n",
    "\n",
    "for a_line in set(my_text['title'].values):\n",
    "    a_line = re.sub('[^ ㄱ-ㅣ가-힣0-9]+', ' ', a_line) # 한글, 숫자 이외는 모두 공백으로\n",
    "    a_line = re.sub(no_meaning, '', a_line)            # 의미없는 문자 삭제\n",
    "    a_line = re.sub('\\s+', ' ', a_line)                # 잉여 스페이스를 공백 하나로\n",
    "    my_text_clean += [a_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = konlpy.tag.Hannanum()\n",
    "\n",
    "my_words = []\n",
    "for a_line, n in zip(my_text_clean, range(len(my_text_clean))):\n",
    "    #print(n, end=\" \")\n",
    "    if a_line == \" \":\n",
    "        #print(\"패스\")\n",
    "        continue\n",
    "    my_words += hannanum.nouns(a_line)\n",
    "\n",
    "my_words_2 = [\"\"]\n",
    "for a_word in my_words:\n",
    "    if len(a_word) > 1:\n",
    "        my_words_2[0] += \" \" + a_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('최저임금 인상', 99),\n",
       " ('가격 인상', 57),\n",
       " ('4월 소비자물가', 28),\n",
       " ('김밥 자장면', 26),\n",
       " ('편의점 도시락', 25),\n",
       " ('김영록 장관', 23),\n",
       " ('소비자물가 상승', 21),\n",
       " ('송재희 지소연', 21),\n",
       " ('식품 기업', 20),\n",
       " ('창업 인큐베이팅', 20)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2                                                            # Can be changed to a number equal or larger than 2.\n",
    "n_min = n\n",
    "n_max = n\n",
    "n_gram_type = 'word'                                             # n-Gram with words.\n",
    "vectorizer = CountVectorizer(ngram_range=(n_min,n_max), analyzer = n_gram_type)\n",
    "\n",
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()            # Get the n-Grams as a list.\n",
    "n_gram_cts = vectorizer.transform(my_words_2).toarray()             #  The output is an array of array.\n",
    "n_gram_cts = list(n_gram_cts[0])                                 # Convert into a simple list.\n",
    "\n",
    "res_dict = {}\n",
    "for z, zz in zip(n_grams, n_gram_cts):\n",
    "    res_dict[z] = zz\n",
    "res_dict = sorted(res_dict.items(), key=(lambda x: x[1]), reverse=True)\n",
    "res_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    n = 0\n",
    "    print(text, \"--->\", end=\" \")\n",
    "    for i in range(len(res_dict)):\n",
    "        if text == res_dict[i][0].split(\" \")[0]:\n",
    "            print(res_dict[i][0].split(\" \")[1], end=\" \")\n",
    "            n += 1\n",
    "        if n == 5:\n",
    "            break\n",
    "    print(\"\")\n",
    "    \n",
    "def predict_arr(arr):\n",
    "    n = range(len(arr))\n",
    "    for i, j in zip(arr, n):\n",
    "        print(j+1, \"위 : \", end=\"\")\n",
    "        predict(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 위 : 상승 ---> 최저임금 정부 즉석식품 편의점 6개월 \n",
      "2 위 : 프랜차이즈 ---> 창업 업체 박람회 창업박람회 21 \n",
      "3 위 : 최저임금 ---> 인상 여파 영향 편승 부메랑 \n",
      "4 위 : 메뉴 ---> 출시 13종 개발 1위 가격 \n",
      "5 위 : 식품 ---> 기업 산업 생필품 분야 가격 \n",
      "6 위 : 업체 ---> 대표 10곳 사용 77 가격 \n",
      "7 위 : 창업 ---> 인큐베이팅 아이템 시장 지원 브랜드 \n",
      "8 위 : 최저임금 ---> 인상 여파 영향 편승 부메랑 \n",
      "9 위 : 정부 ---> 최저임금 1년 가격 오징어 체감물가 \n",
      "10 위 : 브랜드 ---> 경복궁 가정 대상 리포터 미스터보쌈 \n"
     ]
    }
   ],
   "source": [
    "predict_arr(['상승', '프랜차이즈', '최저임금', '메뉴', '식품', \n",
    "             '업체', '창업', '최저임금', '정부', '브랜드'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의료보건비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = pd.read_excel(\"의료보건비.xlsx\", encoding='CP949')\n",
    "\n",
    "no_meaning = \"건강보험|건강|보험|의료|의료비|병원|병원비\"\n",
    "my_text_clean = []\n",
    "\n",
    "for a_line in set(my_text['title'].values):\n",
    "    a_line = re.sub('[^ ㄱ-ㅣ가-힣0-9]+', ' ', a_line) # 한글, 숫자 이외는 모두 공백으로\n",
    "    a_line = re.sub(no_meaning, '', a_line)            # 의미없는 문자 삭제\n",
    "    a_line = re.sub('\\s+', ' ', a_line)                # 잉여 스페이스를 공백 하나로\n",
    "    my_text_clean += [a_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = konlpy.tag.Hannanum()\n",
    "\n",
    "my_words = []\n",
    "for a_line, n in zip(my_text_clean, range(len(my_text_clean))):\n",
    "    #print(n, end=\" \")\n",
    "    if a_line == \" \":\n",
    "        #print(\"패스\")\n",
    "        continue\n",
    "    my_words += hannanum.nouns(a_line)\n",
    "\n",
    "my_words_2 = [\"\"]\n",
    "for a_word in my_words:\n",
    "    if len(a_word) > 1:\n",
    "        my_words_2[0] += \" \" + a_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('문재 케어', 204),\n",
       " ('보장성 강화', 139),\n",
       " ('고위험 임산부', 55),\n",
       " ('9988 출시', 45),\n",
       " ('국민 일산', 45),\n",
       " ('농협생명 9988', 43),\n",
       " ('임산부 지원', 38),\n",
       " ('종합 3인실', 38),\n",
       " ('상복부 초음파', 36),\n",
       " ('가입 가능', 34)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2                                                            # Can be changed to a number equal or larger than 2.\n",
    "n_min = n\n",
    "n_max = n\n",
    "n_gram_type = 'word'                                             # n-Gram with words.\n",
    "vectorizer = CountVectorizer(ngram_range=(n_min,n_max), analyzer = n_gram_type)\n",
    "\n",
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()            # Get the n-Grams as a list.\n",
    "n_gram_cts = vectorizer.transform(my_words_2).toarray()             #  The output is an array of array.\n",
    "n_gram_cts = list(n_gram_cts[0])                                 # Convert into a simple list.\n",
    "\n",
    "res_dict = {}\n",
    "for z, zz in zip(n_grams, n_gram_cts):\n",
    "    res_dict[z] = zz\n",
    "res_dict = sorted(res_dict.items(), key=(lambda x: x[1]), reverse=True)\n",
    "res_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    n = 0\n",
    "    print(text, \"--->\", end=\" \")\n",
    "    for i in range(len(res_dict)):\n",
    "        if text == res_dict[i][0].split(\" \")[0]:\n",
    "            print(res_dict[i][0].split(\" \")[1], end=\" \")\n",
    "            n += 1\n",
    "        if n == 5:\n",
    "            break\n",
    "    print(\"\")\n",
    "    \n",
    "def predict_arr(arr):\n",
    "    n = range(len(arr))\n",
    "    for i, j in zip(arr, n):\n",
    "        print(j+1, \"위 : \", end=\"\")\n",
    "        predict(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 위 : 지원 ---> 확대 협약 사업 강화 내년 \n",
      "2 위 : 문재 ---> 케어 대통령 정부 헬스케어 강아지분양 \n",
      "3 위 : 정부 ---> 1년 출범 지원 대납 보장성 \n",
      "4 위 : 의협 ---> 건정심 비대위 복지부 문재 문케 \n",
      "5 위 : 환자 ---> 급증 지원 사랑 실손 행복 \n",
      "6 위 : 복지부 ---> 의협 장관 권고 보장성 상복부 \n",
      "7 위 : 보장성 ---> 강화 강화대책 확대 강화정책 대책 \n",
      "8 위 : 국민공단 ---> 이사장 이달 채용 일산장 부산중부지사 \n",
      "9 위 : 국민 ---> 일산 부담 10명 의견 종합계획 \n",
      "10 위 : 공단 ---> 이사장 일산 상임감사 올해 정책연구원장 \n"
     ]
    }
   ],
   "source": [
    "predict_arr(['지원', '문재', '정부', '의협', '환자', \n",
    "             '복지부', '보장성', '국민공단', '국민', '공단'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의류비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = pd.read_excel(\"의류비.xlsx\", encoding='CP949')\n",
    "\n",
    "no_meaning = \"의류|패션\"\n",
    "my_text_clean = []\n",
    "\n",
    "for a_line in set(my_text['title'].values):\n",
    "    a_line = re.sub('[^ ㄱ-ㅣ가-힣0-9]+', ' ', a_line) # 한글, 숫자 이외는 모두 공백으로\n",
    "    a_line = re.sub(no_meaning, '', a_line)            # 의미없는 문자 삭제\n",
    "    a_line = re.sub('\\s+', ' ', a_line)                # 잉여 스페이스를 공백 하나로\n",
    "    my_text_clean += [a_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = konlpy.tag.Hannanum()\n",
    "\n",
    "my_words = []\n",
    "for a_line, n in zip(my_text_clean, range(len(my_text_clean))):\n",
    "    #print(n, end=\" \")\n",
    "    if a_line == \" \":\n",
    "        #print(\"패스\")\n",
    "        continue\n",
    "    my_words += hannanum.nouns(a_line)\n",
    "\n",
    "my_words_2 = [\"\"]\n",
    "for a_word in my_words:\n",
    "    if len(a_word) > 1:\n",
    "        my_words_2[0] += \" \" + a_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('공항 포토', 108),\n",
       " ('시각 코스닥', 37),\n",
       " ('코스닥 코스닥', 37),\n",
       " ('코스닥 현재', 37),\n",
       " ('엑소 세훈', 32),\n",
       " ('미모 공항', 29),\n",
       " ('비주얼 공항', 27),\n",
       " ('화보 공개', 27),\n",
       " ('최대 50', 25),\n",
       " ('트롬 미니워시', 24)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2                                                            # Can be changed to a number equal or larger than 2.\n",
    "n_min = n\n",
    "n_max = n\n",
    "n_gram_type = 'word'                                             # n-Gram with words.\n",
    "vectorizer = CountVectorizer(ngram_range=(n_min,n_max), analyzer = n_gram_type)\n",
    "\n",
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()            # Get the n-Grams as a list.\n",
    "n_gram_cts = vectorizer.transform(my_words_2).toarray()             #  The output is an array of array.\n",
    "n_gram_cts = list(n_gram_cts[0])                                 # Convert into a simple list.\n",
    "\n",
    "res_dict = {}\n",
    "for z, zz in zip(n_grams, n_gram_cts):\n",
    "    res_dict[z] = zz\n",
    "res_dict = sorted(res_dict.items(), key=(lambda x: x[1]), reverse=True)\n",
    "res_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    n = 0\n",
    "    print(text, \"--->\", end=\" \")\n",
    "    for i in range(len(res_dict)):\n",
    "        if text == res_dict[i][0].split(\" \")[0]:\n",
    "            print(res_dict[i][0].split(\" \")[1], end=\" \")\n",
    "            n += 1\n",
    "        if n == 5:\n",
    "            break\n",
    "    print(\"\")\n",
    "    \n",
    "def predict_arr(arr):\n",
    "    n = range(len(arr))\n",
    "    for i, j in zip(arr, n):\n",
    "        print(j+1, \"위 : \", end=\"\")\n",
    "        predict(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 위 : 공항 ---> 포토 트와이스 코웨 방탄소년단 업계 \n",
      "2 위 : 포토 ---> 엑소 현아 안소희 구혜선 세븐틴 \n",
      "3 위 : 브랜드 ---> 모카썸 론칭 모델 쿠스토 데이즈 \n",
      "4 위 : 화보 ---> 공개 공항 심소영 포토 고준희 \n",
      "5 위 : 여름 ---> 상품 화보 기획전 바캉스 뷰티 \n",
      "6 위 : 업계 ---> 최초 진출 1분기 냉감 사업 \n",
      "7 위 : 개최 ---> 포토 수익금 롯데 오마이걸 삼성 \n",
      "8 위 : 출시 ---> 포토 봇물 백화점 용량 글로벌 \n",
      "9 위 : 진행 ---> 포토 2018 업계 화보 공승연 \n",
      "10 위 : 백화점 ---> 겨울 마트 매장 명품 아웃도어 \n"
     ]
    }
   ],
   "source": [
    "predict_arr(['공항', '포토', '브랜드', '화보', '여름', \n",
    "             '업계', '개최', '출시', '진행', '백화점'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
